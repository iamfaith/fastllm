```
docker run --gpus all \
-it --name trt \
--ipc=host \
--ulimit memlock=-1 \
--ulimit stack=67108864 \
-v ${PWD}:/workspace/ \
nvcr.io/nvidia/pytorch:23.04-py3
```


```
docker run --gpus all \
-it --rm \
--ipc=host \
--ulimit memlock=-1 \
--ulimit stack=67108864 \
-v ${PWD}:/home/faith/fastllm \
nvcr.io/nvidia/pytorch:23.04-py3
```


``` sh
cd fastllm
mkdir build
cd build
cmake .. -DUSE_CUDA=ON # å¦‚æœä¸ä½¿ç”¨GPUç¼–è¯‘ï¼Œé‚£ä¹ˆä½¿ç”¨ cmake .. -DUSE_CUDA=OFF
make -j16
cd tools && python setup.py install
```

for win:
export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64/:/usr/local/cuda-11.8/targets/x86_64-linux/lib/:/home/faith/miniconda3/lib/python3.9/site-packages/nvidia/cublas/lib/


## optimization

https://zhuanlan.zhihu.com/p/620331719

https://zhuanlan.zhihu.com/p/619217800


compare time:
https://mdnice.com/writing/9f132e6af4694d0c9a6155d3cb31a8b4

torch.nn.functional.scaled_dot_product_attention

modeling_chatglm.py: CoreAttention


For CUDA tensor inputs, the function will dispatch into one of the following implementations:

1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
2. Memory-Efficient Attention
3. A PyTorch implementation defined in C++

https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html
https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html


https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/fc133e4ffc6275f9d1c3a74ddd10e0a2/scaled_dot_product_attention_tutorial.ipynb


https://github.com/NVIDIA/FasterTransformer/blob/main/docs/gpt_guide.md


https://pytorch.org/blog/accelerating-large-language-models/


scaled_dot_product_attention å¦‚ä½•ç”¨ C++ çš„ cuda é«˜æ•ˆå®ç°ï¼Œè¿™ä¸ªé—®é¢˜æ²¡æœ‰ä¸€ä¸ªç¡®å®šçš„ç­”æ¡ˆï¼Œä¸åŒçš„å®ç°å¯èƒ½ä¼šæœ‰ä¸åŒçš„ä¼˜ç¼ºç‚¹ã€‚ä½ å¯ä»¥å‚è€ƒä¸€äº›å·²æœ‰çš„å®ç°ï¼Œæ¯”å¦‚ï¼š

- [PyTorch ä¸­çš„ ScaledDotProduct.cpp](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/ScaledDotProduct.cpp)
- [FasterTransformer ä¸­çš„ scaled_dot_product_attention.cu](https://github.com/NVIDIA/FasterTransformer/blob/main/src/decoding/scaled_dot_product_attention.cu)
- [PyTorch çš„æ•™ç¨‹ä¸­çš„ (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)

ä¸€èˆ¬æ¥è¯´ï¼Œé«˜æ•ˆå®ç° scaled_dot_product_attention éœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

- åˆ©ç”¨ Tensor Core çš„è®¡ç®—èƒ½åŠ›ï¼Œä½¿ç”¨ FP16 æˆ–è€… INT8 çš„æ•°æ®ç±»å‹
- åˆå¹¶æˆ–è€…èåˆä¸€äº›æ“ä½œï¼Œæ¯”å¦‚ bias fusion, layer normalization fusion, softmax fusion ç­‰
- ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œ IO æ•ˆç‡ï¼Œæ¯”å¦‚ FlashAttention å’Œ Memory-Efficient Attention ç­‰
- æ ¹æ®è¾“å…¥æ•°æ®å’Œæ¨¡å‹å‚æ•°çš„ä¸åŒï¼Œé€‰æ‹©åˆé€‚çš„ç®—æ³•å’Œå‚æ•°

Source: Conversation with Bing, 7/19/2023
(1) torch.nn.functional.scaled_dot_product_attention. https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html.
(2) (Beta) Implementing High-Performance Transformers with .... https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html.
(3) How to Implement Scaled Dot-Product Attention from Scratch .... https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/.


/home/faith/pytorch/aten/src/ATen/native/transformers/attention.cpp




æ‚¨å¥½ï¼Œè¿™æ˜¯Bingã€‚æˆ‘å¯ä»¥å¸®æ‚¨è§£é‡Šä¸€ä¸‹chatGLMä¸­çš„top_kã€top_på’Œtemperatureå‚æ•°çš„å«ä¹‰ã€‚ğŸ˜Š

chatGLMæ˜¯ä¸€ç§åŸºäºGPTç³»åˆ—æ¨¡å‹çš„èŠå¤©ç”Ÿæˆå™¨ï¼Œå®ƒå¯ä»¥æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ç”Ÿæˆè‡ªç„¶å’Œæœ‰è¶£çš„å›å¤ã€‚ä¸ºäº†æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä¸­çš„éšæœºæ€§å’Œå¤šæ ·æ€§ï¼ŒchatGLMæä¾›äº†ä¸€äº›å‚æ•°ï¼Œå¦‚top_kã€top_på’Œtemperatureï¼Œè®©ç”¨æˆ·å¯ä»¥è°ƒæ•´ç”Ÿæˆçš„è´¨é‡å’Œé£æ ¼ã€‚

top_kæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå®ƒè¡¨ç¤ºåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­ï¼Œæ¨¡å‹åªè€ƒè™‘æœ€å¯èƒ½çš„å‰kä¸ªå•è¯ä½œä¸ºå€™é€‰è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œå¦‚æœtop_kè®¾ç½®ä¸º3ï¼Œé‚£ä¹ˆæ¨¡å‹åªä¼šä»æ¦‚ç‡æœ€é«˜çš„ä¸‰ä¸ªå•è¯ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸ºè¾“å‡ºã€‚top_kè¶Šå¤§ï¼Œç”Ÿæˆçš„å¤šæ ·æ€§è¶Šé«˜ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´ä¸€äº›ä¸åˆç†æˆ–ä¸è¿è´¯çš„è¾“å‡ºã€‚top_kè¶Šå°ï¼Œç”Ÿæˆçš„ä¸€è‡´æ€§è¶Šé«˜ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´ä¸€äº›é‡å¤æˆ–æ— èŠçš„è¾“å‡ºã€‚

top_pæ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œå®ƒè¡¨ç¤ºåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­ï¼Œæ¨¡å‹åªè€ƒè™‘æ¦‚ç‡ä¹‹å’Œä¸è¶…è¿‡pçš„æœ€å¯èƒ½çš„å•è¯ä½œä¸ºå€™é€‰è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œå¦‚æœtop_pè®¾ç½®ä¸º0.75ï¼Œé‚£ä¹ˆæ¨¡å‹ä¼šä»æ¦‚ç‡æœ€é«˜çš„å•è¯å¼€å§‹ç´¯åŠ ï¼Œç›´åˆ°æ¦‚ç‡ä¹‹å’Œè¾¾åˆ°æˆ–è¶…è¿‡0.75ä¸ºæ­¢ï¼Œç„¶åä»è¿™äº›å•è¯ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸ºè¾“å‡ºã€‚top_pä¹Ÿå¯ä»¥æ§åˆ¶ç”Ÿæˆçš„å¤šæ ·æ€§å’Œä¸€è‡´æ€§ï¼Œä½†å®ƒæ˜¯åŠ¨æ€çš„ï¼Œå› ä¸ºå®ƒæ ¹æ®æ¯ä¸ªå•è¯çš„æ¦‚ç‡æ¥è°ƒæ•´å€™é€‰åˆ—è¡¨çš„å¤§å°ã€‚top_pé€šå¸¸è®¾ç½®ä¸ºè¾ƒé«˜çš„å€¼ï¼ˆå¦‚0.75ï¼‰ï¼Œç›®çš„æ˜¯é™åˆ¶å¯èƒ½è¢«é‡‡æ ·çš„ä½æ¦‚ç‡å•è¯çš„é•¿å°¾ã€‚

temperatureæ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œå®ƒè¡¨ç¤ºåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­ï¼Œæ¨¡å‹å¯¹æ¯ä¸ªå•è¯çš„æ¦‚ç‡è¿›è¡Œé‡æ–°è°ƒæ•´çš„ç¨‹åº¦ã€‚æ¸©åº¦è¶Šé«˜ï¼Œæ¨¡å‹å¯¹æ¯ä¸ªå•è¯çš„æ¦‚ç‡è¶Šå¹³å‡ï¼Œè¿™æ„å‘³ç€æ›´å¤šçš„éšæœºæ€§å’Œåˆ›é€ æ€§ã€‚æ¸©åº¦è¶Šä½ï¼Œæ¨¡å‹å¯¹æ¯ä¸ªå•è¯çš„æ¦‚ç‡è¶Šåå‘äºåŸå§‹å€¼ï¼Œè¿™æ„å‘³ç€æ›´å°‘çš„éšæœºæ€§å’Œæ›´å¤šçš„ç¡®å®šæ€§ã€‚æ¸©åº¦ä¸º0æ—¶ï¼Œæ¨¡å‹æ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å•è¯ä½œä¸ºè¾“å‡ºã€‚

è¿™äº›å‚æ•°å¯ä»¥æ ¹æ®ä¸åŒçš„ä»»åŠ¡å’Œåœºæ™¯è¿›è¡Œè°ƒæ•´ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„ç”Ÿæˆæ•ˆæœã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ‰§è¡Œæœ‰â€œæ­£ç¡®â€ç­”æ¡ˆçš„ä»»åŠ¡ï¼ˆå¦‚é—®ç­”æˆ–æ‘˜è¦ï¼‰æ—¶ï¼Œè¾ƒä½çš„æ¸©åº¦ï¼ˆå°äº1ï¼‰å’Œè¾ƒå°çš„top_kæˆ–top_pæ›´åˆé€‚ã€‚æ‰§è¡Œéœ€è¦æ›´å¤šåˆ›æ„å’Œå¤šæ ·æ€§çš„ä»»åŠ¡ï¼ˆå¦‚å†™æ•…äº‹æˆ–æ­Œè¯ï¼‰æ—¶ï¼Œè¾ƒé«˜çš„æ¸©åº¦ï¼ˆå¤§äº1ï¼‰å’Œè¾ƒå¤§çš„top_kæˆ–top_pæ›´åˆé€‚ã€‚

å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºè¿™äº›å‚æ•°çš„ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥å‚è€ƒä»¥ä¸‹é“¾æ¥ï¼š
- [chatglmå¸¸ç”¨å‚æ•° ï¼šTop-k, Top-p, Temperature - CSDNåšå®¢](^1^)
- [Top-k & Top-p, Temperature - çŸ¥ä¹](^2^)
- [chatgpt è°ƒç”¨å‚æ•°è§£æ - çŸ¥ä¹](^3^)

å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶æå‡ºã€‚ğŸ˜Š

Source: Conversation with Bing, 7/19/2023
(1) chatglmå¸¸ç”¨å‚æ•° ï¼šTop-k, Top-p, Temperature - CSDNåšå®¢. https://blog.csdn.net/stay_foolish12/article/details/131478570.
(2) Top-k & Top-p, Temperature - çŸ¥ä¹ - çŸ¥ä¹ä¸“æ . https://zhuanlan.zhihu.com/p/613428710.
(3) chatgpt è°ƒç”¨å‚æ•°è§£æ - çŸ¥ä¹ - çŸ¥ä¹ä¸“æ . https://zhuanlan.zhihu.com/p/613262543.



top_k = 1  ç®€å•çš„greedy search

top_p temperature repeat_penalty éƒ½å’Œé‡‡æ ·æœ‰å…³

float repeat_penalty = 1.0f; // é‡å¤æƒ©ç½šç³»æ•°ï¼Œ1.0ä»£è¡¨ä¸æƒ©ç½š
float temperature = 1.0; // æ¸©åº¦å‚æ•°ï¼Œä¸€èˆ¬åœ¨0.1 ~ 1.0ä¹‹é—´ï¼Œè®¾å¤§è¿™ä¸ªå‚æ•°å¯ä»¥å¸¦æ¥ç»“æœçš„å¤šæ ·æ€§